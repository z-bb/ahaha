在spark中，使用抽象类MemoryConsumer来表示需要使用内存的消费者。在这个类中定义了分配，释放以及spill内存数据到磁盘的一些方法或者接口。具体的消费者可以继承MemoryConsumer从而实现具体的行为。因此，在spark task执行过程中，会有各种类型不同，数量不一的具体消费者。如在spark shuffle中使用的ExternalAppendOnlyMap，ExternalSorter等等。
MemoryConsumer会将申请，释放相关内存的工作交由TaskMemoryManager来执行。当一个spark task被分配到Excutor上运行时，会创建一个TaskMemoryManager。在TaskMemoryManager执行分配内存之前，需要首先向MemoryManager进行申请，然后由TaskMemoryManager借助MemoryAllcator执行实际的内存分配。
Executor中的MemoryManager会统一管理内存的使用。由于每个TaskMemoryManager在执行实际的内存分配前，会首先向MemoryManager提出申请。因此MemoryManager会对当前进程使用内存的情况有着全局的了解。
                                               MemoryManager
					                                 |
			 	        -----------------------------|--------------------------------                           
                        |                            |						         |
		        TaskMemoryManager 		      TaskMemoryManager              TaskMemoryManager
		               |                              |                               |
		      ----------------                 ---------------                 ---------------      
	 		  |              |                 |             |                 |             |
	    MemoryConsumer MemoryConsumer   MemoryConsumer MemoryConsumer    MemoryConsumer MemoryConsumer
		
当有多个task同事在Executor上执行时，将会有多个TaskMemoryManager共享MemoryManager管理的内存。每个任务可以分配的内存范围时[1 / (2 * n), 1 / n],其中n时正在运行的task数。因此，多并发运行的task会使得每个Task可以获得的内存变小。
前面提到，到MemoryConsumer中有spill方法，MemoryConsumer申请不到足够的内存时，可以spill当前内存到磁盘，从而避免无节制的使用内存。但是，对于堆内内存的申请和释放实际上是由JVM来管理的。因此，在统计堆内内存的具体使用量时，考虑性能等各方面原因，spark目前采用的是抽样统计的方式来计算MemoryConsumer已经使用的内存，从而造成堆内内存的实际使用量不是特别准确。从而有可能因为不能及时spill而导致oom。
spark shuffle过程
spark shuffle主要分为两个阶段：shuffle write和shuffle read
write阶段大体经历排序，可能的聚合和归并，最终每个写task会产生数据和索引两个文件。其中，数据文件会按照分区进行存储，即相同分区的数据在文件中是连续的，而索引文件记录了每个分区在文件中的起始和结束位置。
而对于shuffle read，首先可能需要通过网络从各个write任务节点获取给定分区的数据，即数据文件中某一段连续的区域，然后经过排序，归并等过程，最终形成计算结果。
对于shuffle write，spark当前有三种实现，具体分别是BypaskkMergeSortShuffleWriter、UnsafeShuffleWriter和SortShuffleWriter。而shuffle read只有一种实现。

shuffle write阶段
BypassMergeSortShuffleWriter
对于BypassMergeSortShuffleWriter的实现，大体实现过程是首先未每个分区创建一个临时分区文件，数据写入对应的分区文件，最终所有的分区文件合并成一个数据文件，并且产生一个索引文件。由于这个过程不做排序，combine等操作，因此对于BypassMergeSortShuffleWriter，总体来说是不怎么耗费内存的。
SortShuffleWriter
SortShuffleWriter是最一般的实现，也是日常使用最频繁的。SortShuffleWriter主要委托ExternalSorter做数据插入，排序，归并聚合以及最终写数据和索引文件的工作。ExternalSorter实现了之前提到的MemoryConsumer接口。
1.对于数据写入，根据是否需要做combiner，数据会被插入到PartitionedAppendOnlyMap这个map或者PartitionedPairBuffer这个数组中。每隔一段时间，单向MemoryManager申请不到足够的内存，或者数据量超过spark.shuffle.spill.numElementsForceSpillThreshold这个阈值时(默认时long的最大值)，就会进行spill内存数据到文件。假设可以源源不断的申请到内存，那么write阶段的所有数据将一直保存在内存中，由此可见，PartitionedAppendOnlyMap或者PartitionedPairBuffer是比较吃内存的。
2.无论是PartitionedAppendOnlyMap还是PartitionedPairBuffer，使用的排序算法是TimSort。在使用该算法是正常情况下使用的临时额外内存空间是很小，但是最坏情况下是n  / 2，其中n表示待排序的数组长度。
3.当插入数据因为申请不到足够的内存将会spill数据到磁盘，在将最终排序结果写入到数据文件之前，需要将内存中的PartitionedAppendMap或PartitionedPairBuffer和已经spill到磁盘的spillfiles进行合并。merge的大体过程如下：
                                                
				---------------------------------->	结果队列
				|          |         |
				|          |         |
			有序队列   有序队列  有序队列
从上图可见，大体差不多就是归并排序的过程，由此可见这个过程是没有太多额外的内存消耗。归并过程中的聚合计算大体也是差不多的过程唯一需要注意的是键值碰撞的情况，即当前输入的各个有序队列的键值的哈希值相同，但是实际的键值不等的情况。在这种情况下，需要额外的空间保存所有的键值不同，但哈希值相同值的中间结果。但是总体上来说，发生这种情况的概率并不是特别大。
4.写数据文件的过程涉及到不同数据流之间的转化，而在流的写入过程中，一般都有缓存，主要由参数spark.shuffle.file.buffle和spark.shuffle.spill.batch控制，总体上这部分开销也不大。
以上分析了SortShuffleWriter write阶段的主要过程，从中可以看出主要内存消耗在写入PartitionedAppendOnlyMap和PartitionPairBiffer这个阶段
UnsafeShuffleWriter
UnsafeShuffleWriter是对SortSnhuffleWriter的优化，大体上也和SortShuffleWriter差不多。从内存角度看，主要差异在以下两点：
一方面，在SortShuffleWriter和PartitionedAppendOnlyMap或者PartitionPairBuffer中，存储的键值或者值的具体类型，也就是java对象，是反序列化过后的数据。而在UnsafeShuffleWriter的ShuffleExternalSorter中数据是序列化以后存储到实际的page中，而且在写入数据过程中会额外吸入长度信息。总体而言，序列化以后数据大小是远远小于序列化之前的数据。
另一方面，UnsafeShuffleWriter中需要额外的存储记录，它保存着分区信息和实际指向序列化后数据的指针。相当于SortShuffleWriter，UnsafeShuffleWriter中这部分存储的开销是额外的。
shuffle read阶段分析
spark shuffle read主要经历从获取数据，序列化流，添加指标统计，可能的聚合计算以及排序等过程
                         ShuffleBlockFetcherIterator(wrappedStreams)
                                           |
                              deserializeStream(recordIter)
                                           |
                              CompletionIterator(metricIter)
                                           |
                                 Iterator(aggregatedIter)
                                           |
                                     Iterator(Sort)
以上计算主要都是迭代进行。在以上步骤中，比较复杂的操作是从远程获取数据聚合和排序操作。
1.数据互殴去分为远程和本地获取。本地获取将直接从本地的BlockManager取数据，而对于远程数据，需要走网络。在远程获取过程中，有相关参数可以控制从远程并发获取数据的大小，则会那个在获取数据的请求数，以及单次数据块请求是否放在内存等参数。具体参数包括spark.reducer.maxSizeInFlight(48m),spark.reducer.maxreqsInFlight,spark.reducer.maxBlockInflightPerAddress和spark.maxRemoteBlockSizeFetchToMem。考虑到数据倾斜的场景，如果map阶段有一个block数据特别的大，默认情况由于spark.maxRemoteBlockSizeFetchToMem没有做限制，所以在这个阶段需要将需要获取的整个block数据放到reduce端的内存中，这个时候是非常耗内存的。可以设置spark.maxRemoteBlockSizeFetchToMem值，如果超过该阈值，可以落盘，避免这种情况的oom。另外，在获取到数据以后，默认情况下会对获取的数据进行校验(参数spark.shuffle.detectCorrupt控制)，这个过程也增加了一定的内存消耗。
2.对于需要聚合和排序的情况，这个过程是借助ExternalAppendOnlyMap实现的。整个插入，spill以及merge的过程和write阶段差不多。总体上，这块也是比较消耗内存的，但是因为有spill操作，当内存不足时，可以将内存数据刷到磁盘，从而释放内存空间。
spark shuffle oom可能性分析
1.首先需要注意Executor端的并发度，多个同时运行的task会共享executor端的内存，使得单个task可使用的内存减少。
2.无论是在Map还是Reduce端，插入数据到内存，排序，归并都是比较占用内存的。因为有spill，理论上不会因为数据倾斜造成oom。但是，由于对堆外对象的分配和释放是由JVM管理的，而spark是通过采样获取已经使用的内存情况，有可能因为采样不准确而不能及时spill，导致oom。
3.在reduce获取数据时，由于数据倾斜，有可能造成单个block的数据非常大，默认情况下需要有足够的内存来保存单个block的数据。因此，此时极有可能因为数据倾斜造成oom，可以设置spark.maxRemoteBlockSizeTetchToMem参数，设置这个参数以后，超过一定的阈值，会自动将数据spill到磁盘，此时便可以避免因为数据倾斜造成oom的情况。
4.在 Reduce 获取数据后，默认情况会对数据流进行解压校验（参数 spark.shuffle.detectCorrupt）。正如在代码注释中提到，由于这部分没有 Spill 到磁盘操作，也有很大的可性能会导致 OOM。



























